{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which one's the real data?\n",
    "\n",
    "In the previous notebook, we found that it is difficult to determine a good algorithmic model for biological connectomes.\n",
    "And how easy it is to get misled - to think that a result found in the data is indication of an underlying algorithm, when in reality it is only a meaningless and misleading side effect.\n",
    "\n",
    "Here, we are using this insight for a little game.\n",
    "\n",
    "I prepared data related to a number of connectomes. One of them is the actual data from human brain tissue. Published as:\n",
    "Peng et al., 2024. Science. Directed and acyclic synaptic connectivity in the human layer 2-3 cortical microcircuit.\n",
    "\n",
    "The others are stochastically generated \"fakes\", built by various different algorithms. Can you tell which one's the real one?\n",
    "To do this, we analyze the connectomes and see if they match a number of findings that have been reported in the paper.\n",
    "\n",
    "## Your input required\n",
    "Note that the code cells below are not complete. Your input and a little bot of coding is required at times. Read the instructions carefully!\n",
    "\n",
    "For reference, you can read the paper under the following url:\n",
    "https://doi.org/10.1126/science.adg8828\n",
    "\n",
    "\n",
    "\n",
    "# Loading the data\n",
    "We begin by loading in the data.\n",
    "\n",
    "Obtain the data from [this link](https://openbraininstitute-my.sharepoint.com/:u:/g/personal/michael_reimann_openbraininstitute_org/EcphXv5auVlLilTB2YzYEQsB6EJtcwLakIzc45PDYaDAfA?e=svddAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install Connectome-Utilities\n",
    "!pip install connectome-analysis\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import conntility\n",
    "import connalysis\n",
    "from scipy import sparse\n",
    "\n",
    "use_gdrive = True\n",
    "\n",
    "if use_gdrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Assumes a shortcut to the shared drive has been placed in your Drive.\n",
    "    fn_data = \"/content/drive/MyDrive/NSC6085_Student_Share/April08/data/mystery_con_mats.h5\"\n",
    "else:\n",
    "    # Alternatively, if the GDrive method does not work, you can download the file separately and place it into the local file system.\n",
    "    # Obtain from the link listed above.\n",
    "    fn_data = \"./mystery_con_mats.h5\"\n",
    "\n",
    "data = conntility.ConnectivityGroup.from_h5(fn_data)\n",
    "\n",
    "print(\"Loaded {0} sample connectivity matrices.\".format(len(data.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a ConnectivityGroup object. That is, a bundle of connectivity matrices that can be analyzed together.\n",
    "The matrices are grouped into different groups: \"mystery_1\" to \"mystery_7\". Each has been generated by a different algorithm, one of them is the original.\n",
    "\n",
    "The .index tells you what each matrix belongs to.\n",
    "\n",
    "The second level of the index numbers the instance of the matrix. That is, we do not have a full regional connectome available in this dataset. Instead, in the paper, many different brain slices were probed by the authors. This resulted in several hundred sampled connectomes, each with only 8-12 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the regular indexing operation to access individual matrices.\n",
    "\n",
    "the .vertices are the properties associated with each vertex, i.e., neuron, such as their locations and distance from the pia.\n",
    "\n",
    ".array depicts the adjacency matrix: Zero values indicate absence of a connection, nonzero values indicate the strength of a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = data[\"mystery_1\", 0]\n",
    "display(mat.array, mat.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert them to a *networkx* graph and use that for example for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "\n",
    "def draw_graph(graph, coords=[\"coordinate_1\", \"coordinate_2\"], **kwargs):\n",
    "      pos = dict([(k, [v[_c] for _c in coords])\n",
    "                  for k, v in graph.nodes.items()])\n",
    "      networkx.draw(graph, pos=pos, **kwargs)\n",
    "\n",
    "graph = data[\"mystery_1\", 0].to_networkx()\n",
    "draw_graph(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which one's the original?\n",
    "\n",
    "### Part 1: The mean amplitude of connections is 0.64 mV\n",
    "We know that in the original data the mean amplitude is 0.64 mV. We can test which of the mystery connectomes match this.\n",
    "\n",
    "First, we write a function that takes as inputs:\n",
    " - The adjacency matrix of a connectome as a scipy.sparse matrix\n",
    " - A pandas.DataFrame of the node properties\n",
    "and returns a Series of connection amplitude values. We reference this function is a basic analysis recipe so that it can be conveniently applied to all matrices.\n",
    "\n",
    "The we test the extracted distributions to the expected mean, separately for each type of matrix (.groupby) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_values(mat, nodes):\n",
    "    vals = pandas.Series(mat.data)\n",
    "    return vals[~numpy.isnan(vals)]\n",
    "\n",
    "analysis = {\n",
    "    \"analyses\": {\n",
    "        \"connection_strengths\": {\n",
    "            \"source\": edge_values,\n",
    "            \"output\": \"Series\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Apply the analysis function to all matrices in the group.\n",
    "result = data.analyze(analysis)\n",
    "\n",
    "# Wrapper for a statistical test of amplitudes against the expected value\n",
    "mean_strength_matches = lambda samples: scipy.stats.ttest_1samp(samples, 0.64).pvalue >= 0.05\n",
    "# Compare distributions separately for all matrix_types.\n",
    "display(result[\"connection_strengths\"].groupby(\"matrix_type\").apply(mean_strength_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The connection probability is 0.158\n",
    "\n",
    "We know the mean connection probability of the data (0.158). Similar to the above, we extract connection probabilities and compare the distribution of results to this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_probability(mat, nodes):\n",
    "    # Fill in a function that returns the connection probability as a single, float value between 0 and 1.\n",
    "    # Warning: There are empty (0 nodes) matrices in the data.analyze\n",
    "    return numpy.nan\n",
    "\n",
    "\n",
    "analysis[\"analyses\"][\"connection_probability\"] = {\n",
    "    \"source\": connection_probability,\n",
    "    \"output\": \"Value\"\n",
    "}\n",
    "# Apply the analysis functions to all matrices in the group.\n",
    "result = data.analyze(analysis)\n",
    "\n",
    "# Wrapper for a statistical test of amplitudes against the expected value\n",
    "mean_prob_matches = lambda samples: scipy.stats.ttest_1samp(samples, 0.158).pvalue >= 0.05\n",
    "# Compare distributions separately for all matrix_types.\n",
    "display(result[\"connection_probability\"].dropna().groupby(\"matrix_type\").apply(mean_strength_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The connection probability does not have a bias for reciprocal connections\n",
    "\n",
    "Many connectomes, e.g. rat and mouse local cortical circuitry have been reported to have an overexpression of reciprocal connections. That is, if between a pair of neurons a connection in one direction exists, then the probability that a connection in the other direction exists as well is higher than expected. \n",
    "\n",
    "This is, however, reportedly not the case for the original human connectivity samples.\n",
    "\n",
    "We test this here in the following way:\n",
    "  1. For each matrix type, count the number of neuron pairs, connections and reciprocal connections\n",
    "  2. Based on this, for each matrix type calculate its individual connection probability\n",
    "  3. Based on this, calculate the expected distribution of the number of reciprocal connections based on null hypothesis (binomial)\n",
    "  4. Based on this, evaluate the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_connections(mat, nodes):\n",
    "    # Return the number of connections in the connection matrix\n",
    "    return numpy.nan\n",
    "\n",
    "def count_pairs(mat, nodes):\n",
    "    # Return the number of ordered neuron pairs.\n",
    "    # Note: This means both the pair (i, j) and (j, i) are counted!\n",
    "    return numpy.nan\n",
    "\n",
    "# Add to the list of analyses\n",
    "analysis[\"analyses\"][\"pair_count\"] = {\n",
    "    \"source\": count_pairs,\n",
    "    \"output\": \"Value\"\n",
    "}\n",
    "analysis[\"analyses\"][\"connection_count\"] = {\n",
    "    \"source\": count_connections,\n",
    "    \"output\": \"Value\"\n",
    "}\n",
    "# To count the number of reciprocal connections we do not need a new function.\n",
    "# We simply apply the connection counting function to the reciprocal connectivity\n",
    "analysis[\"analyses\"][\"reciprocal_count\"] = {\n",
    "    \"source\": count_connections,\n",
    "    \"decorators\": [  # This \"decorator\" means that the analysis function will be applied to the matrix containing only reciprocal connections.\n",
    "        {\"name\": \"for_bidirectional_connectivity\"}\n",
    "    ],\n",
    "    \"output\": \"Value\"\n",
    "}\n",
    "# Apply the analysis functions to all matrices in the group.\n",
    "result = data.analyze(analysis)\n",
    "# Concatenate the three counts\n",
    "result = pandas.concat([result[\"pair_count\"],\n",
    "                        result[\"connection_count\"],\n",
    "                        result[\"reciprocal_count\"]], axis=1, \n",
    "                       keys=[\"pairs\", \"total\", \"reciprocal\"])\n",
    "# This results in a DataFrame with matrix types as rows, the three counts as columns:\n",
    "#\tpairs\ttotal\treciprocal\n",
    "# matrix_type\t\t\t\n",
    "# mystery_1\t7200\t[...]\n",
    "# [...]\n",
    "# mystery_7\t7200    [...]\n",
    "sum_results = result.groupby([\"matrix_type\"]).sum()\n",
    "\n",
    "# We define a function that returns True if the null hypothesis that connections in either direction are formed\n",
    "# statistically independently cannot be rejected at the specified significance level.\n",
    "# The test statistic we use is the total count of reciprocal connections.\n",
    "# The null hypothesis is that this number is binomally distributed with a probability of the square of the basic connection probability\n",
    "def test_rec_count(row, thresh=0.05):\n",
    "    from scipy.stats import binom\n",
    "    num_pairs = row[\"pairs\"]\n",
    "    num_connections = row[\"total\"]\n",
    "    num_reciprocal = row[\"reciprocal\"]\n",
    "\n",
    "    # DEFINE THE APPROPRIATE NULL MODEL HERE!\n",
    "    #distr = binom([...])  # Reciprocal connections are statistically independent. Using binomial distribution\n",
    "    co_left = distr.isf(1.0 - thresh / 2.0)  # Cutoff for the left tail of the distribution. Value beyond this occur with p=thresh/2\n",
    "    co_right = distr.isf(thresh / 2.0)  # Cutoff for the right tail of the distribution.\n",
    "    # If the actual value is between the values, hypothesis cannot be rejected\n",
    "    return row[\"reciprocal\"] >= co_left and row[\"reciprocal\"] <= co_right\n",
    "\n",
    "\n",
    "# Apply\n",
    "display(sum_results.apply(test_rec_count, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The connection probability is distance dependent\n",
    "Many connectomes, e.g. rat and mouse local cortical circuitry have been reported to have a distance-depdendent connection probability.\n",
    "This is also the case for the original human data.\n",
    "\n",
    "In this exercise, we shall evaluate this in a very simple way: We test whether the connection probability for pairs with a soma distance between 0 and 100 um is significantly higher than the connection probability above 100 um.\n",
    "\n",
    "The distance can be calculated by using the columns \"coordinate_1\", \"coordinate_2\", \"coordinate_3\" of the `nodes` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define: A function that returns the connection probability, but only\n",
    "# for pairs with a soma distance > min_dist and <= max_dist. The second condition is\n",
    "# ignored if max_dist is None\n",
    "def connection_probability_within(mat, nodes, min_dist=0, max_dist=None,\n",
    "                                  props=[\"coordinate_1\", \"coordinate_2\", \"coordinate_3\"]):\n",
    "    assert min_dist >= 0\n",
    "    assert (max_dist is None) or (max_dist > min_dist)\n",
    "    from scipy.spatial.distance import (pdist, squareform)\n",
    "    D = squareform(pdist(nodes[props]))  # N x N numpy.array of pairwise distances\n",
    "    #[ FILL IN THE REST]\n",
    "\n",
    "\n",
    "# Add to the list of analyses\n",
    "analysis[\"analyses\"][\"proximal_probability\"] = {\n",
    "    \"source\": connection_probability_within,\n",
    "    \"output\": \"Value\",\n",
    "    \"kwargs\": {\n",
    "        \"min_dist\": 0,\n",
    "        \"max_dist\": 100\n",
    "    }\n",
    "}\n",
    "analysis[\"analyses\"][\"distal_probability\"] = {\n",
    "    \"source\": connection_probability_within,\n",
    "    \"output\": \"Value\",\n",
    "    \"kwargs\": {\n",
    "        \"min_dist\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply the analysis functions to all matrices in the group.\n",
    "result = data.analyze(analysis)\n",
    "# Concatenate the three counts\n",
    "result = pandas.concat([result[\"proximal_probability\"],\n",
    "                        result[\"distal_probability\"]],\n",
    "                        axis=1, \n",
    "                       keys=[\"proximal\", \"distal\"])\n",
    "\n",
    "# Here, define the appropriate statistical test\n",
    "def test_con_prob_is_different(dframe, thresh=0.05):\n",
    "    p1 = dframe[\"proximal\"]\n",
    "    p2 = dframe[\"distal\"]\n",
    "    #[...]\n",
    "\n",
    "\n",
    "# Apply with the appropriate pandas statement\n",
    "#final_result = result....\n",
    "display(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The connection probability is directional\n",
    "For this connectome a certain directionality has been reported.\n",
    "That is, connections are more likely from more superficial neurons to deeper neurons than the other way around.\n",
    "\n",
    "The node property \"piadistance\" can be used for that purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_analyses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
