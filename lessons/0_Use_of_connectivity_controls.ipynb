{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity analysis and stochastic controls\n",
    "\n",
    "The structure of a connectome is often characterized by reporting the values of connectivity metrics, where a given metric captures some aspect of its organization. At its simplest, this can be for example the connection probability, but more complex measures are also applied.\n",
    "\n",
    "Formally, such reports can confirm or (more often) reject a connectivity algorithm to be a good model of a connectome. This is, because for simple algorithms the ranges of values reached are well known. Therefore, a value outside the range would reject the model. \n",
    "\n",
    "However, there can still be many other, very simple algorithms that can explain the results. It is important to investigate them before coming to strong conclusions.\n",
    "\n",
    "We will illustrate this idea in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install Connectome-Utilities\n",
    "!pip install connectome-analysis\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import conntility\n",
    "import connalysis\n",
    "from scipy import sparse\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common neighbor bias\n",
    "\n",
    "It has been observed that connectivity in brain networks is biased for connections between neurons with many _common neighbors_. A \"neighor\" of a neuron in this context is another neuron that is connected to the first; a \"common neighbor\" of a pair of neurons is a third neuron that is connected to both of them.\n",
    "\n",
    "Perin et al., 2012, PNAS found the following:\n",
    "\n",
    "![image.png](../images/perin_synaptic.png)\n",
    "\n",
    "In panel B we see that there are more common neighbors for pairs than \"expected\". Expectation in this context is the distribution of the number of common neighbors between pairs in an \"Erdos-Renyi\" graph. What Erdos-Renyi means will be explained further below.\n",
    "\n",
    "In panel C we see that the probability that the pair is connected strongly depends on the number of common neighbors between them.\n",
    "\n",
    "### The meaning of the measure\n",
    "\n",
    "The phenomenon is often interpreted as evidence of connection clustering. I.e., of the presence of small groups of neurons that are much more tightly interconnected than expected. (In fact, it is closely related to another measure called \"clustering coefficient\"). \n",
    "\n",
    "Formally, the measure demonstrates a statistical dependence between the existence of one connection and another. The existence of the connections to the common neighbors influences the probability that the connection between the pair exists. \n",
    "\n",
    "### Investigate\n",
    "Here, we investigate this proposed phenomenon in some stochastically-generated connectomes and a real connectome.\n",
    "\n",
    "We begin by implementing a function that analyzes how much connection probability increases with common neighbors.\n",
    "It takes as input the representation of the connectome as a graph as input. Specifically, its 'adjacency matrix', represented as a scipy.sparse matrix. That is a N x N matrix, where N is the number of neurons in the connectome, where the entry at [i, j] is True iff there is a connection from neuron #i to neuron #j.\n",
    "\n",
    "An additional input is the direction of connectivity considered for neighbors. Previously, we defined a neuron that is connected to another neuron, but did not specify the direction of the connection: Is it innervated by or innervating the other neuron. In this function, this is made explicit, considering either \"afferent\" or \"efferent\" neighbors. The final input is the minimum number of pairs required for a valid connection probability. If, for a given number of common neighbors fewer than that number of pairs are found, no connection probability sample is recorded.\n",
    "\n",
    "The function returns a pandas.Series with the number of common neigbors as index and the corresponding connection probability as values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_probability_vs_cn(m, direction=\"efferent\", min_num_smpl=100):\n",
    "    # Convert True/False to 1.0/0.0.\n",
    "    m = m.astype(float).tocsr()\n",
    "    # We will below assume that efferent neighbors be considered. For the afferent case we simply transpose.\n",
    "    if direction == \"afferent\":\n",
    "        m = m.transpose()\n",
    "    elif direction != \"efferent\":\n",
    "        raise ValueError(\"Unknow value for direction: {0}\".format(direction))\n",
    "    \n",
    "    # We calculcate a mtrix where the entry at [i,j] is the number of common neighbors between neuron #i and #j.\n",
    "    CN = m * m.transpose()\n",
    "    # From sparse to dense representatiojn\n",
    "    CN = numpy.array(CN.todense())\n",
    "    # Entries along the main diagonal are meaningless. To avoid them affecting the results we set them to NaN.\n",
    "    numpy.fill_diagonal(CN, numpy.nan)\n",
    "\n",
    "    # Iterate over the possible number of common neighbors\n",
    "    max_cn = int(numpy.nanmax(CN))\n",
    "    p_per_cn = []\n",
    "    for cn in range(max_cn + 1):\n",
    "        # Are there enough pairs with that number of common neighbors?\n",
    "        if (CN == cn).sum() >= min_num_smpl:\n",
    "            # Then calculate the connection probability for the pairs and report it\n",
    "            p = m[CN == cn].mean()\n",
    "            p_per_cn.append((cn, p))\n",
    "    df = pandas.DataFrame(p_per_cn, columns=[\"cn\", \"p\"])\n",
    "    return df.set_index(\"cn\")[\"p\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Erdos-Renyi or ER graph is a very simple model. It is defined by only two parameters, its size, i.e. number of nodes (neurons), and its connection probability. An edge exists between any single pair with the specified probability and this process is statistically independent. \n",
    "\n",
    "Below, we call the two parameters \"n\" and \"p\" and use a functions in the \"connalysis\" package to generate a random instance. If you wish, you can write your own implementation instead, it does not take much code.\n",
    "\n",
    "Then, we analyze the common neigbor bias of the results. What result would we expect? As mentioned above in biological neuronal networks we observe an increase of connection probability with common neighbors that is not observed in the Erdos-Renyi networks. In fact, in Erdos-Renyi, we expect no increase at all. Based on the description above, each edge exists with the specified probability and that process is independent of neighboring edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_per_cn = {}\n",
    "\n",
    "n = 1000\n",
    "p = 0.03\n",
    "min_num_smpl = 100\n",
    "\n",
    "mat_er = connalysis.randomization.run_ER(n, p).tocsr()\n",
    "\n",
    "p_per_cn[\"Erdos-Renyi\"] = connection_probability_vs_cn(mat_er)\n",
    "\n",
    "\n",
    "for k, v in p_per_cn.items():\n",
    "    plt.plot(v, marker=\"o\", label=k)\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Number of common neighbors\")\n",
    "plt.gca().set_ylabel(\"Connection probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we see no particular increase or decrease, beyond some minor stochastic variability that is expected.\n",
    "The connection probability hovers around the value we prescribed earlier (i.e., \"p\") everywhere.\n",
    "\n",
    "## Common neighbors in distance-dependent connectivity\n",
    "Let's see how this plays out in a different type of stochastic network. We build one with distance-dependent connection probability. It is constructed according to the following rule: Connections between pairs are formed statistically independently; a connection exists with a probability that depends on the distance between the pair according to the following formula:\n",
    "P(i, j) = p_scale * exp(-p_exponent * D(i, j))\n",
    "\n",
    "We begin by setting values for the two parameters (p_scale, p_exponent) and plotting the connection probability function.\n",
    "\n",
    "Additionally, \"spatial_scale\" defines the overall spatial dimensions used. To define distances between neurons, each neuron is associated with a random location inside a cube of that size.\n",
    "\n",
    "Note: Units for locations and distances are um.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_scale = 400.0\n",
    "p_scale = 0.5\n",
    "p_exponent = 0.015\n",
    "ndims = 3\n",
    "\n",
    "\n",
    "example_dists = numpy.linspace(0, 600, 100)\n",
    "plt.plot(example_dists, p_scale * numpy.exp(-p_exponent * example_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the random neuron locations and build an instance of the stochastic network. Once again, feel free to implement the function generating the network yourself.\n",
    "\n",
    "Upon analyzing its common neighbor bias, we see that connection probability increases quite drastically with common neighbor count!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrn_locs = numpy.random.rand(n, ndims) * spatial_scale\n",
    "mat_dd = connalysis.randomization.run_DD2(n, p_scale, p_exponent, nrn_locs).tocsr()\n",
    "\n",
    "p_per_cn[\"Distance-dependent\"] = connection_probability_vs_cn(mat_dd)\n",
    "\n",
    "for k, v in p_per_cn.items():\n",
    "    plt.plot(v, marker=\"o\", label=k)\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Number of common neighbors\")\n",
    "plt.gca().set_ylabel(\"Connection probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is puzzling: As stated above, this is evidence of statistical dependence between connections. But we know for a fact that in this model connections are formed independently, by construction.\n",
    "\n",
    "Additionally, it is interpreted as evidence for the presence of clusters, but we know that connectivity is homogeneous with no determined clusters.\n",
    "\n",
    "Why is that?\n",
    "\n",
    "While connections are formed independently, their probabilities are not uniform. A pair of neurons with many common neighbors is more likely to be right next to each other than on opposite ends of the volume. Hence, it is also more likely to be connected than average.\n",
    "\n",
    "Below, we begin by calculating the probability distributions of distances of connected and unconnected pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "dist_bin_borders = numpy.linspace(1E-4, 650, 101)\n",
    "dist_bin_centers = 0.5 * (dist_bin_borders[:-1] + dist_bin_borders[1:])\n",
    "\n",
    "D = distance.pdist(nrn_locs)\n",
    "p_per_bin = p_scale * numpy.exp(-p_exponent * dist_bin_centers) # P(connected | distance)\n",
    "distr_of_distances = numpy.histogram(D, bins=dist_bin_borders)[0] / len(D) # P(distance)\n",
    "overall_p = mat_dd.mean() # P(connected)\n",
    "\n",
    "# P(distance | connected) = P(connected | distance) * P(distance) / P(connected)\n",
    "dist_cond_connected = p_per_bin * distr_of_distances / overall_p\n",
    "dist_cond_unconnected = (1.0 - p_per_bin) * distr_of_distances / (1.0 - overall_p)\n",
    "\n",
    "# We plot the distributions of distances\n",
    "plt.plot(dist_bin_centers, dist_cond_connected,\n",
    "         label=\"For connected pairs\")\n",
    "plt.plot(dist_bin_centers, dist_cond_unconnected,\n",
    "         label=\"For unconnected pairs\")\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Distance (um)\"); plt.gca().set_ylabel(\"P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a pair of neurons, A and B. If A and B are connected, their distance follows the blue curve above, otherwise the orange curve.\n",
    "\n",
    "We introduce a third neuron C that is connected to B:\n",
    "\n",
    "A -?-> B --> C\n",
    "\n",
    "Hence, the distance between B and C follows the blue curve.\n",
    "\n",
    "From elementary geometry, we know that the distance between A and C is constrained by the sum of distances between A and B, and B and C:\n",
    "\n",
    "D(A, C) <= D(A, B) + D(A, C)\n",
    "\n",
    "We can numerically calculate that upper limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all combinations of D(A,B) and D(A,C) their sum\n",
    "dist_sum_samples = dist_bin_centers.reshape((-1, 1)) + dist_bin_centers.reshape((1, -1))\n",
    "# For all combinations of D(A,B) and D(A,C) their probability, if A and B are connected (=product of probabilities)\n",
    "dist_sum_prob_ab_connected = dist_cond_connected.reshape((-1, 1)) * dist_cond_connected.reshape((1, -1))\n",
    "# For all combinations of D(A,B) and D(A,C) their probability, if A and B are unconnected\n",
    "dist_sum_prob_ab_unconnected = dist_cond_connected.reshape((-1, 1)) * dist_cond_unconnected.reshape((1, -1))\n",
    "\n",
    "sum_prob_connected = []\n",
    "sum_prob_unconnected = []\n",
    "for a, b in zip(dist_bin_borders[:-1], dist_bin_borders[1:]):\n",
    "    in_bin = (dist_sum_samples >= a) & (dist_sum_samples < b)\n",
    "    sum_prob_connected.append(\n",
    "        dist_sum_prob_ab_connected[in_bin].sum()\n",
    "    )\n",
    "    sum_prob_unconnected.append(\n",
    "        dist_sum_prob_ab_unconnected[in_bin].sum()\n",
    "    )\n",
    "\n",
    "# We plot the distributions of distances\n",
    "plt.plot(dist_bin_centers, sum_prob_connected,\n",
    "         label=\"For A connected to B\")\n",
    "plt.plot(dist_bin_centers, sum_prob_unconnected,\n",
    "         label=\"For A not connected to B\")\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"D(A,B) + D(B,C) (um)\"); plt.gca().set_ylabel(\"P\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is only an upper limit for D(A, C), we see that it is lower if A and B are connected. \n",
    "\n",
    "Therefore, if A and B are connected, A and C are more likely to be connected!\n",
    "\n",
    "This would turn C into a common neighbor of A and B. Such an effect increases the number of common neighbors found between connected pairs. \n",
    "\n",
    "Furthermore, it also demonstrates stochastic dependence between connections: The presence or absence of A -> C has been shown to depend on A -> B existing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks with long-tailed degree distributions\n",
    "\n",
    "Biological neuronal networks have been demonstrated to have long-tailed degree distributions. (Degree refers to the number of connections of a neuron. Out-degree to its number of outgoing connections, in-degree to the incoming connections.)\n",
    "\n",
    "That is, the number of connections they form follow a distribution such as the geometric distribution.\n",
    "\n",
    "We build a network according to that principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import geom\n",
    "\n",
    "# We build a geometric probability distribution for degrees. \n",
    "# The mean of the distribution should be the expected degree of the Erdos-Renyi network we built above.\n",
    "# This makes the networks overall comparable.\n",
    "# The expected degree is easy to calculate as simply p * (n - 1).\n",
    "distr = geom(1 / (p * (n - 1)))\n",
    "# For each neuron a randomly draw target degree.\n",
    "degs = distr.rvs(n)\n",
    "\n",
    "# For each neuron, we randomly pick the generated number from all possible partners.\n",
    "indices = [numpy.random.choice(numpy.setdiff1d(numpy.arange(n), _i), _deg, replace=False)\n",
    "           for _i, _deg in enumerate(degs)]\n",
    "# Turn the result into a sparse matrix representation.\n",
    "indptr = numpy.cumsum([0] + [len(_idx) for _idx in indices])\n",
    "mat_degs = sparse.csr_matrix((numpy.ones(indptr[-1]), numpy.hstack(indices), indptr))\n",
    "\n",
    "p_per_cn[\"Degree-distribution\"] = connection_probability_vs_cn(mat_degs)\n",
    "\n",
    "for k, v in p_per_cn.items():\n",
    "    plt.plot(v, marker=\"o\", label=k)\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Number of common neighbors\")\n",
    "plt.gca().set_ylabel(\"Connection probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see an increase with common neighbor count.\n",
    "\n",
    "Hypothesis: If a pair of neurons has many common neighbors, they are both more likely to be neurons with a high degree. This also makes them more likely to be connected to each other.\n",
    "\n",
    "Based on this hypothesis we can make a prediction. If you read the code generating the stochastic network closely, you may notice that it generates a geometrically distributed out-degree, but the in-degree is not long-tailed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdegs = numpy.array(mat_degs.sum(axis=1))[:, 0]\n",
    "indegs = numpy.array(mat_degs.sum(axis=0))[0]\n",
    "\n",
    "plt.plot(pandas.Series(outdegs).value_counts().sort_index(), label=\"Out-degree\")\n",
    "plt.plot(pandas.Series(indegs).value_counts().sort_index(), label=\"In-degree\")\n",
    "\n",
    "plt.gca().set_xlabel(\"Degree\"); plt.gca().set_ylabel(\"Number of pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, the common neighbor bias should be absent if we consider _afferent_ common neighbors instead of efferent ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_per_cn[\"Degree-distribution - afferent\"] = connection_probability_vs_cn(mat_degs, direction=\"afferent\")\n",
    "\n",
    "for k, v in p_per_cn.items():\n",
    "    plt.plot(v, marker=\"o\", label=k)\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Number of common neighbors\")\n",
    "plt.gca().set_ylabel(\"Connection probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a common neighbor bias can be apparent in distance dependent networks and networks with long-tailed degree distributions, even if they contain no neuron clusters.\n",
    "\n",
    "This demonstrates the following:\n",
    "If a researcher introduces a measure, such as the common neighbor bias, and shows that its value is higher in a network than expected in an Erdos-Renyi network, then they have rejected the Erdos-Renyi network as a model for their network. But they have shown nothing beyond that. They may be tempted to elevate the concepts used in their network measure (such as common neighbors) to a core fundamental principle of a new network algorithm, but that is premature.\n",
    "\n",
    "### Improving the measure\n",
    "\n",
    "Does that mean the common neighbor bias is useless for characterizing a connectome? No, but one has to consider stronger control models than the Erdos-Renyi network and compare them as well.\n",
    "\n",
    "Below, we define a function that takes a connectome (an adjacency matrix) as input and returns a stochastically generated control connectome with the same distance-dependence of connectivity.\n",
    "\n",
    "Then, we build 10 instances and compare their behavior to the original connectome. When we use as the original connectome the distance-dependent connectome we built earlier, we see that it behaves exactly like the controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distance_dependent_control(mat, locations, bin_size_um=20.0):\n",
    "    locs_df = pandas.DataFrame(locations, columns=[\"x\", \"y\", \"z\"])\n",
    "    params = connalysis.modelling.conn_prob_2nd_order_model(mat, locs_df, bin_size_um=bin_size_um)\n",
    "    return connalysis.randomization.run_DD2(len(locations),\n",
    "                                            params[\"exp_model_scale\"],\n",
    "                                           params[\"exp_model_exponent\"],\n",
    "                                           locations)\n",
    "n_controls = 10\n",
    "\n",
    "ctrls = [build_distance_dependent_control(mat_dd, nrn_locs)\n",
    "         for _ in range(n_controls)]\n",
    "\n",
    "res_data = connection_probability_vs_cn(mat_dd)\n",
    "res_ctrl = [\n",
    "    connection_probability_vs_cn(_ctrl)\n",
    "    for _ctrl in ctrls\n",
    "]\n",
    "\n",
    "res_ctrl = pandas.concat(res_ctrl, axis=1)\n",
    "\n",
    "plt.plot(res_data, lw=2, marker=\"o\", color=\"black\", label=\"Data\")\n",
    "plt.plot(res_ctrl, lw=0.5, ls=\"--\", label=\"Control\", color=\"grey\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the degree distributions. A control model that matches the degree distributions of an input connectome is called a \"configuration model\". We use an algorithm from the \"connalysis\" package to generate instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_controls = 10\n",
    "\n",
    "ctrls = [connalysis.randomization.configuration_model(mat_degs)\n",
    "         for _ in range(n_controls)]\n",
    "\n",
    "res_data = connection_probability_vs_cn(mat_degs)\n",
    "res_ctrl = [\n",
    "    connection_probability_vs_cn(_ctrl)\n",
    "    for _ctrl in ctrls\n",
    "]\n",
    "\n",
    "res_ctrl = pandas.concat(res_ctrl, axis=1)\n",
    "\n",
    "plt.plot(res_data, lw=2, marker=\"o\", color=\"black\", label=\"Data\")\n",
    "plt.plot(res_ctrl, lw=0.5, ls=\"--\", label=\"Control\", color=\"grey\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use this to investigate the common neighbor bias in an actual, biological connectome.\n",
    "\n",
    "We analyze the connectome of the [MICrONS project](https://www.microns-explorer.org). That is an electron-microscopic reconstruction of 1 mm^3 of mouse cortical tissue. This dense reconstruction yields all connections between the neurons in the volume, although it is not 100% accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gdrive = True\n",
    "\n",
    "if use_gdrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Assumes a shortcut to the shared drive has been placed in your Drive.\n",
    "    file_path_in_gdrive = \"/content/drive/MyDrive/NSC6085_Student_Share/April08/data/microns_mm3_connectome_v1181.h5\"\n",
    "    microns_data = conntility.ConnectivityMatrix.from_h5(file_path_in_gdrive, 'condensed')\n",
    "else:\n",
    "    # Alternatively, if the GDrive method does not work, you can download the file separately and place it into the local file system.\n",
    "    # Obtain from: https://doi.org/10.5281/zenodo.13849415\n",
    "    microns_data = conntility.ConnectivityMatrix.from_h5(\"./microns_mm3_connectome_v1181.h5\", 'condensed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following extracts a 400 x 400 um subvolume of neurons in layer 4.\n",
    "# This is roughly comparable to the stochastic networks we have built above.\n",
    "x_minmax = (600000, 1000000) # Note: here, units are nm.\n",
    "z_minmax = (650000, 1050000)\n",
    "# First: Consider only neurons in Layer 4\n",
    "microns_l4 = microns_data.index('cell_type').isin([\"L4a\", \"L4b\", \"L4c\"])\n",
    "# How many remain?\n",
    "print(len(microns_l4))\n",
    "# Second: Consider only neurons in the x-interval of interest. (gt = greater than; lt = less than)\n",
    "microns_l4 = microns_l4.index(\"x_nm\").gt(x_minmax[0]).index(\"x_nm\").lt(x_minmax[1])\n",
    "# How many remain?\n",
    "print(len(microns_l4))\n",
    "# Third: Consider only neurons in the z-interval of interest.\n",
    "microns_l4 = microns_l4.index(\"z_nm\").gt(z_minmax[0]).index(\"z_nm\").lt(z_minmax[1])\n",
    "# How many remain?\n",
    "print(len(microns_l4))\n",
    "\n",
    "# The adjacency matrix for the remaining neurons. As used in the algorithms above.\n",
    "mat_mic = microns_l4.matrix.tocsr()\n",
    "# The locations of the remaining neurons. To be used to generate the distance-dependent control.\n",
    "# Note: divide by 1000 to convert from nm to um.\n",
    "locs_mic = microns_l4.vertices[[\"x_nm\", \"y_nm\", \"z_nm\"]].values / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_controls = 3\n",
    "\n",
    "ctrls = [build_distance_dependent_control(mat_mic, locs_mic, bin_size_um=20.0)\n",
    "         for _ in range(n_controls)]\n",
    "\n",
    "res_data = connection_probability_vs_cn(mat_mic)\n",
    "res_ctrl = [\n",
    "    connection_probability_vs_cn(_ctrl)\n",
    "    for _ctrl in ctrls\n",
    "]\n",
    "\n",
    "res_ctrl = pandas.concat(res_ctrl, axis=1)\n",
    "\n",
    "plt.plot(res_data, lw=2, marker=\"o\", color=\"black\", label=\"Data\")\n",
    "plt.plot(res_ctrl, lw=0.5, ls=\"--\", label=\"Control\", color=\"grey\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_controls = 10\n",
    "\n",
    "ctrls = [connalysis.randomization.configuration_model(mat_mic)\n",
    "         for _ in range(n_controls)]\n",
    "\n",
    "res_data = connection_probability_vs_cn(mat_mic)\n",
    "res_ctrl = [\n",
    "    connection_probability_vs_cn(_ctrl)\n",
    "    for _ctrl in ctrls\n",
    "]\n",
    "\n",
    "res_ctrl = pandas.concat(res_ctrl, axis=1)\n",
    "\n",
    "plt.plot(res_data, lw=2, marker=\"o\", color=\"black\", label=\"Data\")\n",
    "plt.plot(res_ctrl, lw=0.5, ls=\"--\", label=\"Control\", color=\"grey\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the biological connectome has a common neighbor bias that is even stronger than explained by its distance dependence and its degree distribution!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_analyses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
